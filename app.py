import streamlit as st
import google.generativeai as genai
from PIL import Image
import pandas as pd
import io
import json
import datetime
import re
import numpy as np

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="Gemini ç¨³å¥é˜Ÿåˆ—è¯Šæ–­",
    page_icon="ğŸš¦",
    layout="wide"
)

# --- åº”ç”¨æ ‡é¢˜å’Œè¯´æ˜ ---
st.title("ğŸš¦ Gemini æ™ºèƒ½è®¢å•è¯Šæ–­å·¥å…· V4.3 (äº¤äº’å¼ç¼–è¾‘ç‰ˆ)")
st.markdown("""
æ¬¢è¿ä½¿ç”¨ï¼æœ¬ç‰ˆæœ¬é‡å¤§å‡çº§ï¼Œç°å·²æ”¯æŒ **å›¾è¡¨å¯¹ç…§** ä¸ **äº¤äº’å¼ç¼–è¾‘**ï¼
- **å›¾è¡¨å¯¹ç…§**ï¼šåœ¨æ¯ä¸ªè¯†åˆ«ç»“æœæ—ç›´æ¥æ˜¾ç¤ºåŸå§‹å›¾ç‰‡ï¼Œæ ¸å¯¹ä¸€ç›®äº†ç„¶ã€‚
- **ç‹¬ç«‹ç¼–è¾‘**ï¼šæ‚¨å¯ä»¥ç›´æ¥åœ¨æ¯ä¸ªè®¢å•çš„ç»“æœè¡¨æ ¼ä¸­ä¿®æ”¹è¯†åˆ«é”™è¯¯çš„æ•°æ®ã€‚
- **æ•°æ®è”åŠ¨**ï¼šæ‚¨åœ¨ä¸Šæ–¹ä»»ä½•è¡¨æ ¼ä¸­æ‰€åšçš„ä¿®æ”¹ï¼Œéƒ½ä¼š **ç«‹å³è‡ªåŠ¨åŒæ­¥** åˆ°ä¸‹æ–¹çš„æ±‡æ€»æ€»è¡¨åŠæœ€ç»ˆçš„Excelå¯¼å‡ºæ–‡ä»¶ä¸­ã€‚
- **å…¨è‡ªåŠ¨å¤„ç†**ï¼šä¾ç„¶ä¿ç•™V4.2çš„å…¨è‡ªåŠ¨æ‰¹å¤„ç†èƒ½åŠ›ã€‚
""")

# --- ä¼šè¯çŠ¶æ€ (Session State) åˆå§‹åŒ– ---
if "file_list" not in st.session_state: st.session_state.file_list = []
if "results" not in st.session_state: st.session_state.results = {}
if "processed_ids" not in st.session_state: st.session_state.processed_ids = []
if "processing_active" not in st.session_state: st.session_state.processing_active = False

# --- å®‰å…¨è®¾ç½® ---
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# --- API å¯†é’¥é…ç½® å’Œ æ¨¡å‹åˆå§‹åŒ– ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    model = genai.GenerativeModel("gemini-1.5-pro-latest", safety_settings=SAFETY_SETTINGS)
except Exception as e:
    st.error(f"åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API å¯†é’¥æˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®: {e}")
    st.stop()

# --- Prompt (ä¿æŒä¸å˜) ---
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„ã€éå¸¸ä¸¥è°¨çš„è®¢å•æ•°æ®å½•å…¥ä¸“å®¶ã€‚
è¯·ä»”ç»†è¯†åˆ«è¿™å¼ æ‰‹å†™è®¢å•å›¾ç‰‡ï¼Œå¹¶æå–æ¯ä¸€è¡Œå•†å“çš„'å“å'ã€'æ•°é‡'ã€'å•ä»·'å’Œ'æ€»ä»·'ã€‚

è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1.  æœ€ç»ˆå¿…é¡»è¾“å‡ºä¸€ä¸ªæ ¼å¼å®Œç¾çš„ JSON æ•°ç»„ã€‚
2.  æ•°ç»„ä¸­çš„æ¯ä¸€ä¸ª JSON å¯¹è±¡ä»£è¡¨ä¸€ä¸ªå•†å“ï¼Œä¸”å¿…é¡»åŒ…å«å››ä¸ªé”®ï¼š "å“å", "æ•°é‡", "å•ä»·", "æ€»ä»·"ã€‚
3.  å¦‚æœå›¾ç‰‡ä¸­çš„æŸä¸€è¡Œç¼ºå°‘æŸä¸ªä¿¡æ¯ï¼ˆä¾‹å¦‚æ²¡æœ‰å†™å•ä»·ï¼‰ï¼Œè¯·å°†å¯¹åº”çš„å€¼è®¾ä¸ºç©ºå­—ç¬¦ä¸² ""ã€‚
4.  å¦‚æœæŸä¸ªæ–‡å­—æˆ–æ•°å­—éå¸¸æ¨¡ç³Šï¼Œæ— æ³•ç¡®å®šï¼Œä¹Ÿè¯·è®¾ä¸ºç©ºå­—ç¬¦ä¸² ""ã€‚
5.  **ä½ çš„å›ç­”å¿…é¡»æ˜¯çº¯ç²¹çš„ã€å¯ä»¥ç›´æ¥è§£æçš„ JSON æ–‡æœ¬**ã€‚ç»å¯¹ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€è¯´æ˜æ–‡å­—ã€æˆ–è€… Markdown çš„ ```json ``` æ ‡è®°ã€‚
"""

# --- æ•°æ®æ¸…æ´—å‡½æ•° (ä¿æŒä¸å˜) ---
def clean_and_convert_to_numeric(value):
    if value is None or not isinstance(value, str) or value.strip() == "": return np.nan
    numbers = re.findall(r'[\d\.]+', str(value))
    if numbers:
        try: return float(numbers[0])
        except (ValueError, IndexError): return np.nan
    return np.nan

# --- æ–‡ä»¶ä¸Šä¼ ä¸é˜Ÿåˆ—ç®¡ç† (ä¿æŒä¸å˜) ---
st.header("STEP 1: ä¸Šä¼ æ‰€æœ‰è®¢å•å›¾ç‰‡")
uploaded_files = st.file_uploader("è¯·åœ¨æ­¤å¤„ä¸Šä¼ ä¸€å¼ æˆ–å¤šå¼ å›¾ç‰‡", type=["jpg", "jpeg", "png"], accept_multiple_files=True)
if uploaded_files:
    new_file_ids = {f.file_id for f in uploaded_files}
    old_file_ids = {f.file_id for f in st.session_state.file_list}
    if new_file_ids != old_file_ids:
        st.session_state.file_list = uploaded_files
        st.session_state.results = {}
        st.session_state.processed_ids = []
        st.session_state.processing_active = False
        st.info("æ£€æµ‹åˆ°æ–°çš„æ–‡ä»¶åˆ—è¡¨ï¼Œå·²é‡ç½®å¤„ç†é˜Ÿåˆ—ã€‚")
        st.rerun()

# --- é˜Ÿåˆ—å¤„ç†æ§åˆ¶ä¸­å¿ƒ (ä¿æŒä¸å˜) ---
if st.session_state.file_list:
    files_to_process = [f for f in st.session_state.file_list if f.file_id not in st.session_state.processed_ids]
    total_count = len(st.session_state.file_list)
    remaining_count = len(files_to_process)
    processed_count = total_count - remaining_count

    st.header("STEP 2: è‡ªåŠ¨å¤„ç†è¯†åˆ«ä»»åŠ¡")
    st.progress(processed_count / total_count if total_count > 0 else 0, text=f"å¤„ç†è¿›åº¦ï¼š{processed_count} / {total_count}")

    col1, col2 = st.columns(2)
    with col1:
        if not st.session_state.processing_active and files_to_process:
            if st.button("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†", use_container_width=True, type="primary"):
                st.session_state.processing_active = True
                st.rerun()
    with col2:
        if st.session_state.processing_active:
            if st.button("â¹ï¸ åœæ­¢å¤„ç†", use_container_width=True):
                st.session_state.processing_active = False
                st.warning("å¤„ç†å·²æ‰‹åŠ¨åœæ­¢ã€‚")
                st.rerun()

    if st.session_state.processing_active and files_to_process:
        next_file_to_process = files_to_process[0]
        with st.spinner(f"æ­£åœ¨è¯†åˆ« {next_file_to_process.name}... (é˜Ÿåˆ—å‰©ä½™ {remaining_count-1} å¼ )"):
            try:
                file_id = next_file_to_process.file_id
                image = Image.open(next_file_to_process).convert("RGB")
                response = model.generate_content([PROMPT_TEMPLATE, image])
                if not response.text or not response.text.strip():
                    raise ValueError("æ¨¡å‹è¿”å›äº†ç©ºå†…å®¹ã€‚å¯èƒ½æ˜¯å›¾ç‰‡è´¨é‡é—®é¢˜æˆ–å®‰å…¨ç­–ç•¥è§¦å‘ã€‚")

                cleaned_text = response.text.strip().removeprefix("```json").removesuffix("```").strip()
                data = json.loads(cleaned_text)
                df = pd.DataFrame.from_records(data)
                df.rename(columns={"æ•°é‡": "è¯†åˆ«æ•°é‡", "å•ä»·": "è¯†åˆ«å•ä»·", "æ€»ä»·": "è¯†åˆ«æ€»ä»·"}, inplace=True)
                
                expected_cols = ["å“å", "è¯†åˆ«æ•°é‡", "è¯†åˆ«å•ä»·", "è¯†åˆ«æ€»ä»·"]
                for col in expected_cols:
                    if col not in df.columns: df[col] = ""

                df['æ•°é‡_num'] = df['è¯†åˆ«æ•°é‡'].apply(clean_and_convert_to_numeric)
                df['å•ä»·_num'] = df['è¯†åˆ«å•ä»·'].apply(clean_and_convert_to_numeric)
                df['æ€»ä»·_num'] = df['è¯†åˆ«æ€»ä»·'].apply(clean_and_convert_to_numeric)
                df['è®¡ç®—æ€»ä»·'] = (df['æ•°é‡_num'] * df['å•ä»·_num']).round(2)
                df['[æŒ‰æ€»ä»·]æ¨ç®—æ•°é‡'] = np.where(df['å•ä»·_num'] != 0, (df['æ€»ä»·_num'] / df['å•ä»·_num']).round(2), np.nan)
                df['[æŒ‰æ€»ä»·]æ¨ç®—å•ä»·'] = np.where(df['æ•°é‡_num'] != 0, (df['æ€»ä»·_num'] / df['æ•°é‡_num']).round(2), np.nan)
                df['çŠ¶æ€'] = np.where(np.isclose(df['è®¡ç®—æ€»ä»·'], df['æ€»ä»·_num']), 'âœ… ä¸€è‡´', 'âš ï¸ éœ€æ ¸å¯¹')
                df.loc[df['è®¡ç®—æ€»ä»·'].isna() | df['æ€»ä»·_num'].isna(), 'çŠ¶æ€'] = 'â” ä¿¡æ¯ä¸è¶³'

                final_cols = ["å“å", "è¯†åˆ«æ•°é‡", "è¯†åˆ«å•ä»·", "è¯†åˆ«æ€»ä»·", "è®¡ç®—æ€»ä»·", "[æŒ‰æ€»ä»·]æ¨ç®—æ•°é‡", "[æŒ‰æ€»ä»·]æ¨ç®—å•ä»·", "çŠ¶æ€"]
                st.session_state.results[file_id] = df[final_cols]
                st.session_state.processed_ids.append(file_id)
            except Exception as e:
                st.error(f"å¤„ç† {next_file_to_process.name} æ—¶å‡ºé”™: {e}")
                st.session_state.processed_ids.append(next_file_to_process.file_id)
                st.session_state.results[next_file_to_process.file_id] = pd.DataFrame([{"å“å": f"è¯†åˆ«å¤±è´¥", "çŠ¶æ€": "âŒ é”™è¯¯"}])
            st.rerun()

    if not files_to_process and total_count > 0:
        if st.session_state.processing_active:
            st.session_state.processing_active = False
            st.success("ğŸ‰ æ‰€æœ‰å›¾ç‰‡å‡å·²è‡ªåŠ¨å¤„ç†å®Œæ¯•ï¼")
            st.balloons()
            st.rerun()
        else:
            st.info("æ‰€æœ‰å›¾ç‰‡å‡å·²å¤„ç†ã€‚è¯·åœ¨ä¸‹æ–¹æŸ¥çœ‹ã€ç¼–è¾‘å’Œå¯¼å‡ºç»“æœã€‚")


# --- âœ… æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ: ç»“æœå±•ç¤ºä¸å¯¼å‡º ---
st.header("STEP 3: å¯¹ç…§å›¾ç‰‡ï¼Œç¼–è¾‘ç»“æœ")

if not st.session_state.results:
    st.info("å°šæœªå¤„ç†ä»»ä½•å›¾ç‰‡ã€‚è¯·å…ˆä¸Šä¼ å¹¶å¼€å§‹å¤„ç†ã€‚")
else:
    # éå†æ‰€æœ‰å·²å¤„ç†çš„æ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºä¸€ä¸ªå¯ç¼–è¾‘çš„åŒºåŸŸ
    for file in st.session_state.file_list:
        if file.file_id in st.session_state.results:
            st.markdown("---")
            st.subheader(f"ğŸ“„ è®¢å•ï¼š{file.name}")
            
            # åˆ›å»ºå·¦å³ä¸¤æ ï¼Œå·¦è¾¹æ”¾å›¾ï¼Œå³è¾¹æ”¾å¯ç¼–è¾‘è¡¨æ ¼
            col_img, col_editor = st.columns([1, 2])

            with col_img:
                st.image(file, caption="åŸå§‹è®¢å•å›¾ç‰‡", use_column_width=True)

            with col_editor:
                # è·å–å½“å‰æ–‡ä»¶çš„DataFrame
                current_df = st.session_state.results[file.file_id]
                
                # ä½¿ç”¨st.data_editorä½¿å…¶å¯ç¼–è¾‘
                # å…³é”®ç‚¹1ï¼šä½¿ç”¨å”¯ä¸€çš„keyï¼Œé˜²æ­¢Streamlitæ··æ·†ä¸åŒçš„ç¼–è¾‘å™¨
                # å…³é”®ç‚¹2ï¼šå°†ç¼–è¾‘å™¨çš„è¾“å‡ºï¼ˆç”¨æˆ·ä¿®æ”¹åçš„DataFrameï¼‰æ•è·åˆ°edited_df
                edited_df = st.data_editor(
                    current_df,
                    key=f"editor_{file.file_id}",
                    num_rows="dynamic",
                    use_container_width=True,
                    # ç¦ç”¨è‡ªåŠ¨è®¡ç®—çš„åˆ—ï¼Œåªå…è®¸ä¿®æ”¹åŸå§‹è¯†åˆ«æ•°æ®
                    disabled=["è®¡ç®—æ€»ä»·", "[æŒ‰æ€»ä»·]æ¨ç®—æ•°é‡", "[æŒ‰æ€»ä»·]æ¨ç®—å•ä»·", "çŠ¶æ€"]
                )

                # å…³é”®ç‚¹3ï¼šå°†ç”¨æˆ·ä¿®æ”¹åçš„DataFrameç«‹å³å†™å›session_state
                # è¿™ä½¿å¾—æ‰€æœ‰åç»­æ“ä½œï¼ˆå¦‚ä¸‹æ–¹çš„æ±‡æ€»è¡¨ï¼‰éƒ½èƒ½è·å–åˆ°æœ€æ–°çš„æ•°æ®
                st.session_state.results[file.file_id] = edited_df

    # --- æ±‡æ€»é¢„è§ˆä¸å¯¼å‡º ---
    st.markdown("---")
    st.header("STEP 4: é¢„è§ˆæ±‡æ€»ç»“æœå¹¶å¯¼å‡º")

    # ä»session_stateä¸­æ”¶é›†æ‰€æœ‰ï¼ˆå¯èƒ½å·²è¢«ç¼–è¾‘è¿‡çš„ï¼‰DataFrame
    all_dfs = [df for df in st.session_state.results.values() if isinstance(df, pd.DataFrame) and 'è¯†åˆ«æ•°é‡' in df.columns]
    
    if all_dfs:
        st.subheader("ç»Ÿä¸€ç»“æœé¢„è§ˆåŒº (æ ¹æ®æ‚¨çš„ä¿®æ”¹å®æ—¶æ›´æ–°)")
        
        # å°†æ‰€æœ‰æœ€æ–°çš„DataFrameåˆå¹¶æˆä¸€ä¸ªæ€»è¡¨
        # å› ä¸ºä¸Šæ–¹å·²ç»å°†ç¼–è¾‘åçš„æ•°æ®å†™å›session_stateï¼Œè¿™é‡Œè‡ªç„¶èƒ½æ‹¿åˆ°æœ€æ–°æ•°æ®
        merged_df = pd.concat(all_dfs, ignore_index=True)
        
        # è¿™é‡Œä½¿ç”¨st.dataframeè¿›è¡Œåªè¯»é¢„è§ˆï¼Œå› ä¸ºç¼–è¾‘æ“ä½œå·²åœ¨ä¸Šæ–¹å®Œæˆ
        st.dataframe(merged_df, use_container_width=True, height=300)
        
        st.subheader("å¯¼å‡ºä¸º Excel æ–‡ä»¶")
        output = io.BytesIO()
        # ä½¿ç”¨æœ€æ–°çš„merged_dfæ¥ç”ŸæˆExcel
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            merged_df.to_excel(writer, index=False, sheet_name='è¯Šæ–­ç»“æœ')
            worksheet = writer.sheets['è¯Šæ–­ç»“æœ']
            for i, col in enumerate(merged_df.columns):
                column_len = max(merged_df[col].astype(str).map(len).max(), len(col)) + 2
                worksheet.set_column(i, i, column_len)

        excel_data = output.getvalue()
        now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"è®¢å•è¯Šæ–­ç»“æœ_{now}.xlsx"
        
        st.download_button(
            label="âœ… ç‚¹å‡»ä¸‹è½½ã€æœ€ç»ˆä¿®æ­£åã€‘çš„Excel",
            data=excel_data,
            file_name=file_name,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
