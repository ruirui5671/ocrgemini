import streamlit as st
import google.generativeai as genai
from PIL import Image
import pandas as pd
import io
import json
import datetime
import re
import numpy as np

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="Gemini ç¨³å¥é˜Ÿåˆ—è¯Šæ–­",
    page_icon="ğŸš¦",
    layout="wide"
)

# --- åº”ç”¨æ ‡é¢˜å’Œè¯´æ˜ ---
st.title("ğŸš¦ Gemini æ™ºèƒ½è®¢å•è¯Šæ–­å·¥å…· V4.1 (å®‰å…¨ç­–ç•¥ä¼˜åŒ–ç‰ˆ)")
st.markdown("""
æ¬¢è¿ä½¿ç”¨ï¼æœ¬ç‰ˆæœ¬å·²ä¼˜åŒ– **å†…å®¹å®‰å…¨ç­–ç•¥** å¹¶å¢å¼ºäº† **é”™è¯¯å¤„ç†**ï¼Œä»¥è§£å†³æ¨¡å‹å¯èƒ½è¿”å›ç©ºå†…å®¹çš„é—®é¢˜ã€‚
- **ä»»åŠ¡é˜Ÿåˆ—**ï¼šä¸Šä¼ çš„æ‰€æœ‰å›¾ç‰‡å°†è¿›å…¥ä¸€ä¸ªâ€œå¾…å¤„ç†â€é˜Ÿåˆ—ã€‚
- **é€ä¸€å¤„ç†**ï¼šç‚¹å‡»â€œå¤„ç†ä¸‹ä¸€å¼ â€æŒ‰é’®ï¼Œåº”ç”¨å°†ä¸€æ¬¡åªè¯†åˆ«ä¸€å¼ å›¾ç‰‡ï¼Œæœ‰æ•ˆé¿å…å†…å­˜æº¢å‡ºã€‚
- **çŠ¶æ€æ¸…æ™°**ï¼šæ—¶åˆ»äº†è§£å¤„ç†è¿›åº¦ï¼Œè¿˜å‰©å¤šå°‘å¼ å¾…å¤„ç†ã€‚
""")

# --- ä¼šè¯çŠ¶æ€ (Session State) åˆå§‹åŒ– ---
if "file_list" not in st.session_state: st.session_state.file_list = []
if "results" not in st.session_state: st.session_state.results = {}
if "processed_ids" not in st.session_state: st.session_state.processed_ids = []

# --- âœ… æ ¸å¿ƒä¿®æ”¹ 1ï¼šé…ç½®å®‰å…¨è®¾ç½® ---
# å°†å®‰å…¨è®¾ç½®çš„é˜ˆå€¼è°ƒæ•´ä¸ºâ€œå…¨éƒ¨å±è”½â€ï¼Œè¿™ä¼šæ”¾å®½å†…å®¹å®¡æŸ¥ï¼Œå‡å°‘å› è¯¯åˆ¤å¯¼è‡´çš„ç©ºè¿”å›
# æ³¨æ„ï¼šè¿™å¹¶ä¸æ„å‘³ç€ä¸å®‰å…¨ï¼Œåªæ˜¯é™ä½äº†æ¨¡å‹â€œè¿‡äºæ•æ„Ÿâ€çš„æ¦‚ç‡
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# --- API å¯†é’¥é…ç½® å’Œ æ¨¡å‹åˆå§‹åŒ– ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    # åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œä¼ å…¥æˆ‘ä»¬å®šä¹‰å¥½çš„å®‰å…¨è®¾ç½®
    model = genai.GenerativeModel("gemini-1.5-pro-latest", safety_settings=SAFETY_SETTINGS)
except Exception as e:
    st.error(f"åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API å¯†é’¥æˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®: {e}")
    st.stop()

# --- Prompt (ä¿æŒä¸å˜ï¼Œä½†ä¸ºäº†å®Œæ•´æ€§ç²˜è´´åœ¨è¿™é‡Œ) ---
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„ã€éå¸¸ä¸¥è°¨çš„è®¢å•æ•°æ®å½•å…¥ä¸“å®¶ã€‚
è¯·ä»”ç»†è¯†åˆ«è¿™å¼ æ‰‹å†™è®¢å•å›¾ç‰‡ï¼Œå¹¶æå–æ¯ä¸€è¡Œå•†å“çš„'å“å'ã€'æ•°é‡'ã€'å•ä»·'å’Œ'æ€»ä»·'ã€‚

è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1.  æœ€ç»ˆå¿…é¡»è¾“å‡ºä¸€ä¸ªæ ¼å¼å®Œç¾çš„ JSON æ•°ç»„ã€‚
2.  æ•°ç»„ä¸­çš„æ¯ä¸€ä¸ª JSON å¯¹è±¡ä»£è¡¨ä¸€ä¸ªå•†å“ï¼Œä¸”å¿…é¡»åŒ…å«å››ä¸ªé”®ï¼š "å“å", "æ•°é‡", "å•ä»·", "æ€»ä»·"ã€‚
3.  å¦‚æœå›¾ç‰‡ä¸­çš„æŸä¸€è¡Œç¼ºå°‘æŸä¸ªä¿¡æ¯ï¼ˆä¾‹å¦‚æ²¡æœ‰å†™å•ä»·ï¼‰ï¼Œè¯·å°†å¯¹åº”çš„å€¼è®¾ä¸ºç©ºå­—ç¬¦ä¸² ""ã€‚
4.  å¦‚æœæŸä¸ªæ–‡å­—æˆ–æ•°å­—éå¸¸æ¨¡ç³Šï¼Œæ— æ³•ç¡®å®šï¼Œä¹Ÿè¯·è®¾ä¸ºç©ºå­—ç¬¦ä¸² ""ã€‚
5.  **ä½ çš„å›ç­”å¿…é¡»æ˜¯çº¯ç²¹çš„ã€å¯ä»¥ç›´æ¥è§£æçš„ JSON æ–‡æœ¬**ã€‚ç»å¯¹ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€è¯´æ˜æ–‡å­—ã€æˆ–è€… Markdown çš„ ```json ``` æ ‡è®°ã€‚
"""
def clean_and_convert_to_numeric(value):
    if value is None or not isinstance(value, str) or value.strip() == "": return np.nan
    numbers = re.findall(r'[\d\.]+', str(value))
    if numbers:
        try: return float(numbers[0])
        except (ValueError, IndexError): return np.nan
    return np.nan

# --- æ–‡ä»¶ä¸Šä¼ ä¸é˜Ÿåˆ—ç®¡ç† ---
st.header("STEP 1: ä¸Šä¼ æ‰€æœ‰è®¢å•å›¾ç‰‡")
uploaded_files = st.file_uploader("è¯·åœ¨æ­¤å¤„ä¸Šä¼ ä¸€å¼ æˆ–å¤šå¼ å›¾ç‰‡", type=["jpg", "jpeg", "png"], accept_multiple_files=True)
if uploaded_files: st.session_state.file_list = uploaded_files

# --- é˜Ÿåˆ—å¤„ç†æ§åˆ¶ä¸­å¿ƒ ---
if st.session_state.file_list:
    files_to_process = [f for f in st.session_state.file_list if f.file_id not in st.session_state.processed_ids]
    
    st.header("STEP 2: é€ä¸€å¤„ç†è¯†åˆ«ä»»åŠ¡")
    
    if not files_to_process:
        st.success("ğŸ‰ æ‰€æœ‰å›¾ç‰‡å‡å·²å¤„ç†å®Œæ¯•ï¼è¯·åœ¨ä¸‹æ–¹æŸ¥çœ‹ã€ç¼–è¾‘å’Œå¯¼å‡ºç»“æœã€‚")
    else:
        total_count = len(st.session_state.file_list)
        remaining_count = len(files_to_process)
        st.info(f"**ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€**ï¼šå…± {total_count} å¼ å›¾ç‰‡ï¼Œè¿˜å‰© **{remaining_count}** å¼ å¾…å¤„ç†ã€‚")
        
        next_file_to_process = files_to_process[0]
        st.subheader(f"ä¸‹ä¸€ä¸ªå¾…å¤„ç†ï¼š**{next_file_to_process.name}**")
        st.image(next_file_to_process, width=200, caption="å¾…å¤„ç†å›¾ç‰‡é¢„è§ˆ")
        
        if st.button("ğŸ‘‰ å¤„ç†è¿™ä¸€å¼ ", use_container_width=True, type="primary"):
            with st.spinner(f"æ­£åœ¨è¯†åˆ« {next_file_to_process.name}..."):
                try:
                    file_id = next_file_to_process.file_id
                    image = Image.open(next_file_to_process).convert("RGB")
                    
                    response = model.generate_content([PROMPT_TEMPLATE, image])
                    
                    # --- âœ… æ ¸å¿ƒä¿®æ”¹ 2ï¼šå¢å¼ºçš„é”™è¯¯å¤„ç† ---
                    # åœ¨è§£æå‰ï¼Œå…ˆæ£€æŸ¥è¿”å›çš„æ–‡æœ¬æ˜¯å¦ä¸ºç©º
                    if not response.text or not response.text.strip():
                        # å¦‚æœæ˜¯ç©ºçš„ï¼ŒæŠ›å‡ºä¸€ä¸ªæ›´å‹å¥½çš„ã€è‡ªå®šä¹‰çš„é”™è¯¯
                        raise ValueError("æ¨¡å‹è¿”å›äº†ç©ºå†…å®¹ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºå›¾ç‰‡è´¨é‡é—®é¢˜æˆ–å†…å®¹å®‰å…¨ç­–ç•¥è¢«è§¦å‘ã€‚è¯·å°è¯•å¦ä¸€å¼ å›¾ç‰‡ã€‚")

                    cleaned_text = response.text.strip().removeprefix("```json").removesuffix("```").strip()
                    data = json.loads(cleaned_text)
                    
                    df = pd.DataFrame.from_records(data)
                    df.rename(columns={"æ•°é‡": "è¯†åˆ«æ•°é‡", "å•ä»·": "è¯†åˆ«å•ä»·", "æ€»ä»·": "è¯†åˆ«æ€»ä»·"}, inplace=True)
                    
                    # ... (è¯Šæ–­é€»è¾‘å’ŒåŸæ¥ä¸€æ ·) ...
                    expected_cols = ["å“å", "è¯†åˆ«æ•°é‡", "è¯†åˆ«å•ä»·", "è¯†åˆ«æ€»ä»·"]
                    for col in expected_cols:
                        if col not in df.columns: df[col] = ""
                    df['æ•°é‡_num'] = df['è¯†åˆ«æ•°é‡'].apply(clean_and_convert_to_numeric)
                    df['å•ä»·_num'] = df['è¯†åˆ«å•ä»·'].apply(clean_and_convert_to_numeric)
                    df['æ€»ä»·_num'] = df['è¯†åˆ«æ€»ä»·'].apply(clean_and_convert_to_numeric)
                    df['è®¡ç®—æ€»ä»·'] = (df['æ•°é‡_num'] * df['å•ä»·_num']).round(2)
                    df['[æŒ‰æ€»ä»·]æ¨ç®—æ•°é‡'] = np.where(df['å•ä»·_num'] != 0, (df['æ€»ä»·_num'] / df['å•ä»·_num']).round(2), np.nan)
                    df['[æŒ‰æ€»ä»·]æ¨ç®—å•ä»·'] = np.where(df['æ•°é‡_num'] != 0, (df['æ€»ä»·_num'] / df['æ•°é‡_num']).round(2), np.nan)
                    df['çŠ¶æ€'] = np.where(np.isclose(df['è®¡ç®—æ€»ä»·'], df['æ€»ä»·_num']), 'âœ… ä¸€è‡´', 'âš ï¸ éœ€æ ¸å¯¹')
                    df.loc[df['è®¡ç®—æ€»ä»·'].isna() | df['æ€»ä»·_num'].isna(), 'çŠ¶æ€'] = 'â” ä¿¡æ¯ä¸è¶³'

                    final_cols = ["å“å", "è¯†åˆ«æ•°é‡", "è¯†åˆ«å•ä»·", "è¯†åˆ«æ€»ä»·", "è®¡ç®—æ€»ä»·", "[æŒ‰æ€»ä»·]æ¨ç®—æ•°é‡", "[æŒ‰æ€»ä»·]æ¨ç®—å•ä»·", "çŠ¶æ€"]
                    st.session_state.results[file_id] = df[final_cols]
                    
                    st.session_state.processed_ids.append(file_id)
                    st.success(f"âœ… {next_file_to_process.name} å¤„ç†æˆåŠŸï¼")
                    st.rerun()

                except Exception as e:
                    # ç°åœ¨ï¼Œè¿™é‡Œçš„eä¼šåŒ…å«æˆ‘ä»¬è‡ªå®šä¹‰çš„ValueErrorä¿¡æ¯ï¼Œæˆ–è€…åŸå§‹çš„JSONDecodeError
                    st.error(f"å¤„ç† {next_file_to_process.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    st.session_state.processed_ids.append(next_file_to_process.file_id)
                    st.session_state.results[next_file_to_process.file_id] = pd.DataFrame([{"å“å": f"è¯†åˆ«å¤±è´¥", "çŠ¶æ€": "âŒ é”™è¯¯"}])
                    st.rerun()

# --- ç»“æœå±•ç¤ºä¸å¯¼å‡º (ä¿æŒä¸å˜) ---
st.header("STEP 3: æŸ¥çœ‹ã€ç¼–è¾‘ä¸å¯¼å‡ºç»“æœ")
# ... (åç»­ä»£ç å’ŒV4.0ä¸€æ ·)
if not st.session_state.results:
    st.info("å°šæœªå¤„ç†ä»»ä½•å›¾ç‰‡ã€‚")
else:
    for file in st.session_state.file_list:
        if file.file_id in st.session_state.results:
            with st.expander(f"ğŸ“„ è®¢å•ï¼š{file.name} (å·²å¤„ç†)", expanded=False):
                st.dataframe(st.session_state.results[file.file_id], use_container_width=True)

    all_dfs = [df for df in st.session_state.results.values() if 'è¯†åˆ«æ•°é‡' in df.columns]
    if all_dfs:
        st.subheader("ç»Ÿä¸€ç¼–è¾‘åŒº")
        merged_df = pd.concat(all_dfs, ignore_index=True)
        edited_df = st.data_editor(merged_df,num_rows="dynamic",use_container_width=True,height=300,disabled=["è®¡ç®—æ€»ä»·", "[æŒ‰æ€»ä»·]æ¨ç®—æ•°é‡", "[æŒ‰æ€»ä»·]æ¨ç®—å•ä»·", "çŠ¶æ€"])
        st.subheader("å¯¼å‡ºä¸º Excel æ–‡ä»¶")
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            edited_df.to_excel(writer, index=False, sheet_name='è¯Šæ–­ç»“æœ')
            writer.sheets['è¯Šæ–­ç»“æœ'].autofit()
        excel_data = output.getvalue()
        now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"è®¢å•è¯Šæ–­ç»“æœ_{now}.xlsx"
        st.download_button(label="âœ… ç‚¹å‡»ä¸‹è½½ã€è¯Šæ–­åã€‘çš„Excel",data=excel_data,file_name=file_name,mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",use_container_width=True)
